#!/usr/bin/env python3
"""
Sistema RAG (Retrieval-Augmented Generation) para o Chatbot ANTAQ
Integra busca sem√¢ntica com gera√ß√£o de respostas via OpenAI
"""

import openai
from typing import List, Dict, Any, Optional, Tuple
import json
import logging
from datetime import datetime
import re
from dataclasses import dataclass
from .vector_store import VectorStoreANTAQ

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ChatMessage:
    """Representa uma mensagem no chat"""
    role: str  # 'user', 'assistant', 'system'
    content: str
    timestamp: datetime = None
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()
        if self.metadata is None:
            self.metadata = {}

class RAGSystemANTAQ:
    """
    Sistema RAG principal para consultas sobre normas ANTAQ
    """
    
    def __init__(
        self,
        openai_api_key: str,
        vector_store: VectorStoreANTAQ,
        model: str = "gpt-4",
        max_context_length: int = 8000,
        temperature: float = 0.1
    ):
        """
        Inicializa o sistema RAG
        
        Args:
            openai_api_key: Chave da API OpenAI
            vector_store: Inst√¢ncia do banco vetorial
            model: Modelo GPT a usar
            max_context_length: Comprimento m√°ximo do contexto
            temperature: Temperatura para gera√ß√£o
        """
        
        self.openai_api_key = openai_api_key
        openai.api_key = openai_api_key
        
        self.vector_store = vector_store
        self.model = model
        self.max_context_length = max_context_length
        self.temperature = temperature
        
        # Hist√≥rico da conversa
        self.conversation_history: List[ChatMessage] = []
        
        logger.info(f"Sistema RAG inicializado com modelo: {model}")
    
    def _create_system_prompt(self) -> str:
        """Cria o prompt do sistema"""
        return """Voc√™ √© um assistente especializado em normas da ANTAQ (Ag√™ncia Nacional de Transportes Aquavi√°rios). 

SUAS RESPONSABILIDADES:
‚Ä¢ Responder perguntas sobre normas, regulamenta√ß√µes e legisla√ß√£o portu√°ria e aquavi√°ria
‚Ä¢ Fornecer informa√ß√µes precisas baseadas apenas no conte√∫do das normas fornecidas
‚Ä¢ Citar sempre as normas espec√≠ficas ao responder
‚Ä¢ Explicar conceitos t√©cnicos de forma clara
‚Ä¢ Alertar quando n√£o h√° informa√ß√£o suficiente nos documentos

DIRETRIZES DE RESPOSTA:
‚Ä¢ Use apenas informa√ß√µes do contexto fornecido
‚Ä¢ Cite o t√≠tulo e c√≥digo das normas relevantes
‚Ä¢ Se n√£o souber algo, seja honesto sobre a limita√ß√£o
‚Ä¢ Forne√ßa respostas estruturadas e organizadas
‚Ä¢ Use linguagem t√©cnica quando apropriado, mas sempre explicativa

FORMATO DE RESPOSTA:
‚Ä¢ Resposta direta √† pergunta
‚Ä¢ Fundamenta√ß√£o legal (citando normas espec√≠ficas)
‚Ä¢ Informa√ß√µes complementares quando relevante
‚Ä¢ Links para documentos quando dispon√≠veis

Responda sempre em portugu√™s brasileiro e seja preciso nas cita√ß√µes."""

    def _extract_query_intent(self, query: str) -> Dict[str, Any]:
        """
        Analisa a inten√ß√£o da consulta para otimizar a busca
        
        Args:
            query: Consulta do usu√°rio
            
        Returns:
            Dicion√°rio com an√°lise da inten√ß√£o
        """
        
        # Palavras-chave para diferentes tipos de consulta
        keywords_mapping = {
            'licenciamento': ['licen√ßa', 'licenciamento', 'autoriza√ß√£o', 'permiss√£o'],
            'tarifas': ['tarifa', 'pre√ßo', 'cobran√ßa', 'taxa', 'valor'],
            'operacional': ['opera√ß√£o', 'funcionamento', 'procedimento', 'processo'],
            'fiscalizacao': ['fiscaliza√ß√£o', 'multa', 'infra√ß√£o', 'penalidade', 'autua√ß√£o'],
            'ambiental': ['ambiental', 'meio ambiente', 'sustentabilidade', 'polui√ß√£o'],
            'seguranca': ['seguran√ßa', 'acidente', 'emerg√™ncia', 'risco'],
            'portuario': ['porto', 'terminal', 'cais', 'ber√ßo', 'atraca√ß√£o'],
            'aquaviario': ['aquavi√°rio', 'navega√ß√£o', 'embarca√ß√£o', 'navio', 'transporte']
        }
        
        # Detectar categoria principal
        query_lower = query.lower()
        categories = []
        
        for category, keywords in keywords_mapping.items():
            if any(keyword in query_lower for keyword in keywords):
                categories.append(category)
        
        # Detectar filtros temporais
        temporal_patterns = [
            r'\b20\d{2}\b',  # Anos
            r'\b(janeiro|fevereiro|mar√ßo|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\b',
            r'\b(jan|fev|mar|abr|mai|jun|jul|ago|set|out|nov|dez)\b'
        ]
        
        temporal_info = []
        for pattern in temporal_patterns:
            matches = re.findall(pattern, query_lower)
            temporal_info.extend(matches)
        
        return {
            'categories': categories,
            'temporal_info': temporal_info,
            'query_type': self._classify_query_type(query_lower),
            'entities': self._extract_entities(query)
        }
    
    def _classify_query_type(self, query: str) -> str:
        """Classifica o tipo de consulta"""
        
        if any(word in query for word in ['o que √©', 'defini√ß√£o', 'conceito', 'significa']):
            return 'definition'
        elif any(word in query for word in ['como', 'procedimento', 'processo', 'etapas']):
            return 'procedure'
        elif any(word in query for word in ['quando', 'prazo', 'data', 'per√≠odo']):
            return 'temporal'
        elif any(word in query for word in ['onde', 'local', 'endere√ßo', 'localiza√ß√£o']):
            return 'location'
        elif any(word in query for word in ['quem', 'respons√°vel', 'compet√™ncia']):
            return 'responsibility'
        elif any(word in query for word in ['quanto', 'valor', 'custo', 'pre√ßo']):
            return 'value'
        else:
            return 'general'
    
    def _extract_entities(self, query: str) -> List[str]:
        """Extrai entidades nomeadas da consulta"""
        
        # Padr√µes para entidades comuns
        patterns = [
            r'\b[A-Z]{2,}\b',  # Siglas
            r'\b\d{1,4}/\d{2,4}\b',  # N√∫meros de normas
            r'\bResolu√ß√£o\s+\d+\b',  # Resolu√ß√µes
            r'\bPortaria\s+\d+\b',  # Portarias
            r'\bDecreto\s+\d+\b'  # Decretos
        ]
        
        entities = []
        for pattern in patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            entities.extend(matches)
        
        return list(set(entities))
    
    def _rerank_results(
        self, 
        query: str, 
        results: List[Dict[str, Any]], 
        intent: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Re-ranqueia resultados baseado na inten√ß√£o da consulta
        
        Args:
            query: Consulta original
            results: Resultados da busca vetorial
            intent: An√°lise da inten√ß√£o
            
        Returns:
            Resultados re-ranqueados
        """
        
        def calculate_relevance_score(result: Dict[str, Any]) -> float:
            """Calcula score de relev√¢ncia"""
            
            base_score = result['similarity']
            
            # Bonus por categoria
            document = result['document'].lower()
            title = result['metadata'].get('titulo', '').lower()
            
            category_bonus = 0
            for category in intent['categories']:
                if category in document or category in title:
                    category_bonus += 0.1
            
            # Bonus por entidades encontradas
            entity_bonus = 0
            for entity in intent['entities']:
                if entity.lower() in document or entity.lower() in title:
                    entity_bonus += 0.15
            
            # Penalty por documentos muito antigos se consulta for recente
            date_penalty = 0
            if result['metadata'].get('assinatura'):
                try:
                    year = int(result['metadata']['assinatura'][:4])
                    current_year = datetime.now().year
                    if current_year - year > 10:
                        date_penalty = -0.05
                except:
                    pass
            
            # Bonus por documentos em vigor
            status_bonus = 0.1 if result['metadata'].get('situacao') == 'Em vigor' else 0
            
            final_score = base_score + category_bonus + entity_bonus + date_penalty + status_bonus
            
            return max(0, min(1, final_score))  # Manter entre 0 e 1
        
        # Aplicar novo score
        for result in results:
            result['relevance_score'] = calculate_relevance_score(result)
        
        # Re-ordenar por relev√¢ncia
        return sorted(results, key=lambda x: x['relevance_score'], reverse=True)
    
    def _format_context(self, results: List[Dict[str, Any]]) -> str:
        """
        Formata o contexto para o prompt do LLM
        
        Args:
            results: Resultados da busca
            
        Returns:
            Contexto formatado
        """
        
        if not results:
            return "Nenhum documento relevante encontrado."
        
        context_parts = []
        
        for i, result in enumerate(results, 1):
            metadata = result['metadata']
            
            context_part = f"""
DOCUMENTO {i}:
T√≠tulo: {metadata.get('titulo', 'N/A')}
C√≥digo: {metadata.get('codigo_registro', 'N/A')}
Assunto: {metadata.get('assunto', 'N/A')}
Data de Assinatura: {metadata.get('assinatura', 'N/A')}
Situa√ß√£o: {metadata.get('situacao', 'N/A')}
Link: {metadata.get('link_pdf', 'N/A')}
Relev√¢ncia: {result.get('relevance_score', result['similarity']):.3f}

CONTE√öDO:
{result['document']}
            """.strip()
            
            context_parts.append(context_part)
        
        return "\n\n" + "="*80 + "\n\n".join(context_parts)
    
    def _create_prompt(
        self, 
        query: str, 
        context: str, 
        conversation_history: List[ChatMessage]
    ) -> List[Dict[str, str]]:
        """
        Cria o prompt completo para o LLM
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto dos documentos relevantes
            conversation_history: Hist√≥rico da conversa
            
        Returns:
            Lista de mensagens formatadas para a API
        """
        
        messages = [
            {"role": "system", "content": self._create_system_prompt()}
        ]
        
        # Adicionar hist√≥rico recente (√∫ltimas 3 intera√ß√µes)
        recent_history = conversation_history[-6:] if len(conversation_history) > 6 else conversation_history
        
        for msg in recent_history:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        # Adicionar contexto e consulta atual
        user_message = f"""
CONTEXTO DOS DOCUMENTOS RELEVANTES:
{context}

PERGUNTA DO USU√ÅRIO:
{query}

Por favor, responda √† pergunta baseando-se exclusivamente nos documentos fornecidos no contexto. 
Cite sempre as fontes espec√≠ficas (t√≠tulo e c√≥digo do documento) ao elaborar sua resposta.
        """.strip()
        
        messages.append({
            "role": "user",
            "content": user_message
        })
        
        return messages
    
    def query(
        self, 
        user_query: str, 
        n_results: int = 8,
        filters: Optional[Dict[str, Any]] = None,
        include_history: bool = True
    ) -> Dict[str, Any]:
        """
        Processa uma consulta do usu√°rio
        
        Args:
            user_query: Pergunta do usu√°rio
            n_results: N√∫mero de documentos para contexto
            filters: Filtros para a busca
            include_history: Se deve incluir hist√≥rico da conversa
            
        Returns:
            Resposta estruturada com metadados
        """
        
        try:
            # Adicionar mensagem do usu√°rio ao hist√≥rico
            user_message = ChatMessage(role="user", content=user_query)
            if include_history:
                self.conversation_history.append(user_message)
            
            # Analisar inten√ß√£o da consulta
            intent = self._extract_query_intent(user_query)
            logger.info(f"Inten√ß√£o detectada: {intent}")
            
            # Busca sem√¢ntica
            search_results = self.vector_store.search(
                query=user_query,
                n_results=n_results,
                filters=filters
            )
            
            if not search_results:
                response_content = """
Desculpe, n√£o encontrei documentos relevantes para sua consulta na base de normas da ANTAQ. 

Algumas sugest√µes:
‚Ä¢ Tente reformular sua pergunta com termos mais espec√≠ficos
‚Ä¢ Verifique se est√° perguntando sobre temas relacionados a transporte aquavi√°rio
‚Ä¢ Use palavras-chave como: licenciamento, tarifas, portos, navega√ß√£o, etc.
                """.strip()
                
                assistant_message = ChatMessage(role="assistant", content=response_content)
                if include_history:
                    self.conversation_history.append(assistant_message)
                
                return {
                    'response': response_content,
                    'sources': [],
                    'metadata': {
                        'intent': intent,
                        'search_results_count': 0,
                        'model_used': self.model
                    }
                }
            
            # Re-ranquear resultados
            reranked_results = self._rerank_results(user_query, search_results, intent)
            
            # Preparar contexto
            context = self._format_context(reranked_results)
            
            # Criar prompt
            messages = self._create_prompt(
                user_query, 
                context, 
                self.conversation_history if include_history else []
            )
            
            # Gerar resposta
            response = openai.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=1500
            )
            
            response_content = response.choices[0].message.content
            
            # Adicionar resposta ao hist√≥rico
            assistant_message = ChatMessage(
                role="assistant", 
                content=response_content,
                metadata={
                    'sources_used': len(reranked_results),
                    'intent': intent
                }
            )
            
            if include_history:
                self.conversation_history.append(assistant_message)
            
            # Preparar fontes
            sources = []
            for result in reranked_results[:5]:  # Top 5 fontes
                metadata = result['metadata']
                sources.append({
                    'titulo': metadata.get('titulo', 'N/A'),
                    'codigo_registro': metadata.get('codigo_registro', 'N/A'),
                    'assunto': metadata.get('assunto', 'N/A'),
                    'situacao': metadata.get('situacao', 'N/A'),
                    'assinatura': metadata.get('assinatura', 'N/A'),
                    'link_pdf': metadata.get('link_pdf', 'N/A'),
                    'relevance_score': result.get('relevance_score', result['similarity'])
                })
            
            return {
                'response': response_content,
                'sources': sources,
                'metadata': {
                    'intent': intent,
                    'search_results_count': len(search_results),
                    'reranked_results_count': len(reranked_results),
                    'model_used': self.model,
                    'temperature': self.temperature,
                    'timestamp': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"Erro ao processar consulta: {e}")
            import traceback
            traceback.print_exc()
            
            error_response = f"Desculpe, ocorreu um erro ao processar sua consulta: {str(e)}"
            
            return {
                'response': error_response,
                'sources': [],
                'metadata': {
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
            }
    
    def clear_history(self):
        """Limpa o hist√≥rico da conversa"""
        self.conversation_history = []
        logger.info("Hist√≥rico da conversa limpo")
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """Retorna o hist√≥rico da conversa"""
        return [
            {
                'role': msg.role,
                'content': msg.content,
                'timestamp': msg.timestamp.isoformat(),
                'metadata': msg.metadata
            }
            for msg in self.conversation_history
        ]
    
    def export_conversation(self, filepath: str):
        """Exporta conversa para arquivo JSON"""
        try:
            history = self.get_conversation_history()
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump({
                    'conversation': history,
                    'export_timestamp': datetime.now().isoformat(),
                    'total_messages': len(history)
                }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Conversa exportada para: {filepath}")
            
        except Exception as e:
            logger.error(f"Erro ao exportar conversa: {e}")

if __name__ == "__main__":
    # Teste b√°sico
    try:
        from chatbot.config.config import OPENAI_API_KEY
    except ImportError:
        print("‚ùå Erro ao importar configura√ß√µes do chatbot")
        exit(1)
    
    if not OPENAI_API_KEY:
        print("‚ùå OPENAI_API_KEY n√£o encontrada no arquivo .env")
        exit(1)
    
    # Inicializar sistema
    vector_store = VectorStoreANTAQ(OPENAI_API_KEY)
    rag_system = RAGSystemANTAQ(OPENAI_API_KEY, vector_store)
    
    # Teste de consulta
    test_query = "Como funciona o licenciamento de terminais portu√°rios?"
    
    print(f"üîç Testando consulta: {test_query}")
    result = rag_system.query(test_query)
    
    print(f"\nüìù RESPOSTA:")
    print(result['response'])
    
    print(f"\nüìö FONTES ({len(result['sources'])}):")
    for i, source in enumerate(result['sources'], 1):
        print(f"{i}. {source['titulo']} (Relev√¢ncia: {source['relevance_score']:.3f})")