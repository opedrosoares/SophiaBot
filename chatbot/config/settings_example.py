#!/usr/bin/env python3
"""
Configura√ß√µes do Chatbot ANTAQ
Copie este arquivo para config.py e ajuste as configura√ß√µes conforme necess√°rio
"""

import os
from pathlib import Path
from dotenv import dotenv_values

# Carregar vari√°veis de ambiente do arquivo .env na raiz do projeto
env_path = Path(__file__).parent.parent.parent / '.env'
if env_path.exists():
    env_vars = dotenv_values(env_path)
    for key, value in env_vars.items():
        os.environ[key] = value

# ===============================
# CONFIGURA√á√ïES OBRIGAT√ìRIAS
# ===============================

# OpenAI API Key (obrigat√≥ria)
# Obtenha em: https://platform.openai.com/api-keys
# Configure no arquivo .env na raiz do projeto
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'sua-chave-openai-aqui')

# ===============================
# CONFIGURA√á√ïES DO MODELO
# ===============================

# Modelo OpenAI a ser usado
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4')

# Temperatura para gera√ß√£o (0.0 = determin√≠stico, 1.0 = criativo)
OPENAI_TEMPERATURE = float(os.getenv('OPENAI_TEMPERATURE', '0.1'))

# M√°ximo de tokens na resposta
OPENAI_MAX_TOKENS = int(os.getenv('OPENAI_MAX_TOKENS', '1500'))

# ===============================
# CONFIGURA√á√ïES DO BANCO VETORIAL
# ===============================

# Diret√≥rio para persistir o banco ChromaDB
CHROMA_PERSIST_DIRECTORY = Path('./chroma_db')

# Nome da cole√ß√£o no ChromaDB
COLLECTION_NAME = 'normas_antaq'

# Configura√ß√µes de embedding
EMBEDDING_MODEL = 'text-embedding-3-small'

# ===============================
# CONFIGURA√á√ïES DE PROCESSAMENTO
# ===============================

# Tamanho dos chunks de texto
CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', '1000'))

# Sobreposi√ß√£o entre chunks
CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP', '200'))

# N√∫mero m√°ximo de resultados na busca
MAX_SEARCH_RESULTS = int(os.getenv('MAX_SEARCH_RESULTS', '15'))

# N√∫mero padr√£o de resultados
DEFAULT_SEARCH_RESULTS = int(os.getenv('DEFAULT_SEARCH_RESULTS', '8'))

# ===============================
# CONFIGURA√á√ïES DA INTERFACE
# ===============================

# Porta do servidor Streamlit
STREAMLIT_SERVER_PORT = int(os.getenv('STREAMLIT_SERVER_PORT', '8501'))

# Endere√ßo do servidor
STREAMLIT_SERVER_ADDRESS = os.getenv('STREAMLIT_SERVER_ADDRESS', 'localhost')

# T√≠tulo da aplica√ß√£o
APP_TITLE = "Chatbot ANTAQ - Consultas sobre Normas"

# √çcone da aplica√ß√£o
APP_ICON = "‚öì"

# ===============================
# CONFIGURA√á√ïES DE CACHE
# ===============================

# Habilitar cache
ENABLE_CACHE = os.getenv('ENABLE_CACHE', 'true').lower() == 'true'

# Tempo de vida do cache (segundos)
CACHE_TTL = int(os.getenv('CACHE_TTL', '3600'))

# ===============================
# CONFIGURA√á√ïES DE LOGGING
# ===============================

# N√≠vel de logging
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')

# Arquivo de log
LOG_FILE = os.getenv('LOG_FILE', 'chatbot_logs.log')

# ===============================
# CONFIGURA√á√ïES AVAN√áADAS
# ===============================

# Habilitar re-ranking de resultados
ENABLE_RERANKING = os.getenv('ENABLE_RERANKING', 'true').lower() == 'true'

# Limite de contexto para o LLM
MAX_CONTEXT_LENGTH = int(os.getenv('MAX_CONTEXT_LENGTH', '8000'))

# Timeout para chamadas da API (segundos)
API_TIMEOUT = int(os.getenv('API_TIMEOUT', '60'))

# ===============================
# CAMINHOS DE ARQUIVOS
# ===============================

# Caminho base do projeto
BASE_DIR = Path(__file__).parent.parent

# Caminho para o arquivo de dados
DATA_PATH = BASE_DIR.parent / 'shared' / 'data' / 'normas_antaq_completo.parquet'

# Diret√≥rio para exports
EXPORT_DIR = BASE_DIR / 'exports'

# Diret√≥rio para logs
LOG_DIR = BASE_DIR / 'logs'

# ===============================
# VALIDA√á√ÉO DE CONFIGURA√á√ïES
# ===============================

def validate_config():
    """
    Valida as configura√ß√µes obrigat√≥rias
    
    Returns:
        tuple: (is_valid, error_messages)
    """
    
    errors = []
    
    # Verificar chave da API
    if not OPENAI_API_KEY or OPENAI_API_KEY == 'sua-chave-openai-aqui':
        errors.append("OPENAI_API_KEY n√£o configurada")
    
    # Verificar arquivo de dados
    if not DATA_PATH.exists():
        errors.append(f"Arquivo de dados n√£o encontrado: {DATA_PATH}")
    
    # Verificar modelo v√°lido
    valid_models = ['gpt-4', 'gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o']
    if OPENAI_MODEL not in valid_models:
        errors.append(f"Modelo inv√°lido: {OPENAI_MODEL}. Use um dos: {valid_models}")
    
    # Verificar valores num√©ricos
    if CHUNK_SIZE <= 0:
        errors.append("CHUNK_SIZE deve ser maior que 0")
    
    if CHUNK_OVERLAP < 0:
        errors.append("CHUNK_OVERLAP deve ser >= 0")
    
    if CHUNK_OVERLAP >= CHUNK_SIZE:
        errors.append("CHUNK_OVERLAP deve ser menor que CHUNK_SIZE")
    
    return len(errors) == 0, errors

# ===============================
# CONFIGURA√á√ïES DE DESENVOLVIMENTO
# ===============================

# Modo debug
DEBUG = os.getenv('DEBUG', 'false').lower() == 'true'

# Recarregar dados sempre (desenvolvimento)
FORCE_RELOAD_DATA = os.getenv('FORCE_RELOAD_DATA', 'false').lower() == 'true'

# Mostrar informa√ß√µes detalhadas
VERBOSE = os.getenv('VERBOSE', 'false').lower() == 'true'

if __name__ == "__main__":
    # Teste de configura√ß√£o
    print("üîß Validando configura√ß√µes...")
    
    is_valid, errors = validate_config()
    
    if is_valid:
        print("‚úÖ Todas as configura√ß√µes s√£o v√°lidas!")
        print(f"   ‚Ä¢ Modelo: {OPENAI_MODEL}")
        print(f"   ‚Ä¢ Chunk size: {CHUNK_SIZE}")
        print(f"   ‚Ä¢ Resultados padr√£o: {DEFAULT_SEARCH_RESULTS}")
        print(f"   ‚Ä¢ Dados: {DATA_PATH}")
    else:
        print("‚ùå Erros de configura√ß√£o encontrados:")
        for error in errors:
            print(f"   ‚Ä¢ {error}")
        exit(1)