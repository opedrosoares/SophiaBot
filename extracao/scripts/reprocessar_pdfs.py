#!/usr/bin/env python3
"""
Script para reprocessar PDFs que falharam na extra√ß√£o
Usa estrat√©gias mais robustas e melhor tratamento de erros
"""

import pandas as pd
import requests
import time
import logging
from urllib.parse import urljoin, urlparse
import os
from Scrap import PDFExtractor, SophiaANTAQScraper

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analisar_pdfs_falhados(arquivo):
    """
    Analisa PDFs que falharam na extra√ß√£o
    """
    print(f"üîç AN√ÅLISE DE PDFs FALHADOS")
    print("=" * 35)
    
    df = pd.read_parquet(arquivo)
    
    # Identifica PDFs falhados
    falhados = df[
        (df['link_pdf'].notna()) & 
        (df['conteudo_pdf'].str.len().eq(0) | df['conteudo_pdf'].isna())
    ]
    
    print(f"üìã Total de normas: {len(df)}")
    print(f"üìÑ Com link PDF: {df['link_pdf'].notna().sum()}")
    print(f"‚ùå PDFs falhados: {len(falhados)}")
    
    # Analisa tipos de erro
    if 'erro_extracao' in df.columns:
        erros = falhados['erro_extracao'].value_counts()
        print(f"\nüìä TIPOS DE ERRO:")
        for erro, count in erros.head(10).items():
            print(f"   ‚Ä¢ {erro}: {count}")
    
    return falhados

def testar_url_pdf(url, session):
    """
    Testa se a URL realmente retorna um PDF
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = session.get(url, headers=headers, timeout=30, allow_redirects=True)
        
        if response.status_code == 200:
            content_type = response.headers.get('content-type', '').lower()
            
            if 'pdf' in content_type:
                return True, 'PDF v√°lido'
            elif 'html' in content_type:
                return False, 'Retorna HTML (poss√≠vel erro 404)'
            else:
                return False, f'Content-Type desconhecido: {content_type}'
        else:
            return False, f'HTTP {response.status_code}'
            
    except Exception as e:
        return False, f'Erro de conex√£o: {str(e)}'

def tentar_urls_alternativas(url_original):
    """
    Tenta varia√ß√µes da URL original
    """
    urls_alternativas = [url_original]
    
    # Remove par√¢metros de query
    parsed = urlparse(url_original)
    if parsed.query:
        url_sem_query = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        urls_alternativas.append(url_sem_query)
    
    # Tenta com/sem www
    if 'www.' in parsed.netloc:
        url_sem_www = url_original.replace('www.', '')
        urls_alternativas.append(url_sem_www)
    else:
        url_com_www = url_original.replace('://', '://www.')
        urls_alternativas.append(url_com_www)
    
    # Tenta varia√ß√µes de extens√£o
    if url_original.endswith('.pdf'):
        url_sem_ext = url_original[:-4]
        urls_alternativas.extend([url_sem_ext, url_sem_ext + '.PDF'])
    
    return list(set(urls_alternativas))  # Remove duplicatas

def reprocessar_pdf_robusto(pdf_url, session, pdf_extractor):
    """
    Tenta extrair PDF com estrat√©gias mais robustas
    """
    print(f"üîß Testando URL: {pdf_url}")
    
    # Testa URL original
    is_valid, status = testar_url_pdf(pdf_url, session)
    if is_valid:
        print(f"   ‚úÖ URL v√°lida: {status}")
        resultado = pdf_extractor.extract_pdf_content(pdf_url)
        if resultado.get('conteudo_pdf'):
            return resultado, pdf_url
    
    print(f"   ‚ùå URL inv√°lida: {status}")
    
    # Tenta URLs alternativas
    urls_alternativas = tentar_urls_alternativas(pdf_url)
    print(f"   üîÑ Tentando {len(urls_alternativas)-1} URLs alternativas...")
    
    for url_alt in urls_alternativas[1:]:  # Pula a original
        print(f"      Testando: {url_alt}")
        is_valid, status = testar_url_pdf(url_alt, session)
        
        if is_valid:
            print(f"      ‚úÖ URL alternativa v√°lida: {status}")
            resultado = pdf_extractor.extract_pdf_content(url_alt)
            if resultado.get('conteudo_pdf'):
                return resultado, url_alt
        else:
            print(f"      ‚ùå URL alternativa inv√°lida: {status}")
    
    # Nenhuma URL funcionou
    return {
        'conteudo_pdf': '',
        'metodo_extracao': '',
        'tamanho_pdf': 0,
        'paginas_extraidas': 0,
        'erro_extracao': 'Todas as URLs testadas falharam'
    }, None

def reprocessar_pdfs_falhados(arquivo, max_pdfs=None):
    """
    Reprocessa PDFs que falharam com estrat√©gias mais robustas
    """
    print(f"\nüîÑ REPROCESSAMENTO ROBUSTO DE PDFs FALHADOS")
    print("=" * 50)
    
    df = pd.read_parquet(arquivo)
    
    # Identifica PDFs falhados
    falhados = df[
        (df['link_pdf'].notna()) & 
        (df['conteudo_pdf'].str.len().eq(0) | df['conteudo_pdf'].isna())
    ]
    
    if len(falhados) == 0:
        print("‚úÖ Nenhum PDF falhado encontrado!")
        return df
    
    # Limita quantidade se especificado
    if max_pdfs and len(falhados) > max_pdfs:
        falhados = falhados.head(max_pdfs)
        print(f"üéØ Processando apenas {max_pdfs} PDFs (de {len(df[df['link_pdf'].notna()])} falhados)")
    else:
        print(f"üéØ Processando {len(falhados)} PDFs falhados")
    
    # Cria sess√£o e extrator
    scraper = SophiaANTAQScraper(delay=1.0, extract_pdf_content=False)
    pdf_extractor = PDFExtractor(scraper.session, timeout=60)  # Timeout maior
    
    # Processa cada PDF falhado
    sucessos = 0
    erros = 0
    urls_corrigidas = 0
    
    print(f"\nüì• Iniciando reprocessamento...")
    start_time = time.time()
    
    for idx, (index, row) in enumerate(falhados.iterrows(), 1):
        codigo = row['codigo_registro']
        titulo = row['titulo'][:40] + "..."
        pdf_url = row['link_pdf']
        
        print(f"\nüìÑ [{idx}/{len(falhados)}] ID {codigo}: {titulo}")
        
        try:
            # Tenta reprocessar com estrat√©gias robustas
            resultado, url_corrigida = reprocessar_pdf_robusto(pdf_url, scraper.session, pdf_extractor)
            
            # Atualiza o DataFrame
            df.loc[index, 'conteudo_pdf'] = resultado.get('conteudo_pdf', '')
            df.loc[index, 'metodo_extracao'] = resultado.get('metodo_extracao', '')
            df.loc[index, 'tamanho_pdf'] = resultado.get('tamanho_pdf', 0)
            df.loc[index, 'paginas_extraidas'] = resultado.get('paginas_extraidas', 0)
            df.loc[index, 'erro_extracao'] = resultado.get('erro_extracao', '')
            
            # Se usou URL corrigida, atualiza o link
            if url_corrigida and url_corrigida != pdf_url:
                df.loc[index, 'link_pdf'] = url_corrigida
                urls_corrigidas += 1
                print(f"   üîó URL corrigida: {url_corrigida}")
            
            if resultado.get('conteudo_pdf'):
                chars = len(resultado['conteudo_pdf'])
                metodo = resultado['metodo_extracao']
                print(f"   ‚úÖ Extra√≠do: {chars} chars via {metodo}")
                sucessos += 1
            else:
                erro = resultado.get('erro_extracao', 'Erro desconhecido')
                print(f"   ‚ùå Erro: {erro}")
                erros += 1
            
        except Exception as e:
            print(f"   ‚ùå Exce√ß√£o: {e}")
            df.loc[index, 'erro_extracao'] = f"Exce√ß√£o: {str(e)}"
            erros += 1
        
        # Pausa entre PDFs
        time.sleep(1.0)  # Pausa maior para n√£o sobrecarregar
        
        # Salva progresso a cada 5 PDFs
        if idx % 5 == 0:
            backup_file = f"backups/backup_reprocessamento_{int(time.time())}.parquet"
            df.to_parquet(backup_file, index=False)
            print(f"   üíæ Progresso salvo em: {backup_file}")
    
    # Estat√≠sticas finais
    elapsed_time = time.time() - start_time
    
    print(f"\nüìä REPROCESSAMENTO CONCLU√çDO:")
    print(f"   ‚è±Ô∏è  Tempo total: {elapsed_time/60:.1f} minutos")
    print(f"   ‚úÖ Sucessos: {sucessos}")
    print(f"   ‚ùå Erros: {erros}")
    print(f"   üîó URLs corrigidas: {urls_corrigidas}")
    print(f"   üìà Taxa de sucesso: {sucessos/(sucessos+erros)*100:.1f}%")
    
    return df

def main():
    """
    Fun√ß√£o principal
    """
    print("üîÑ REPROCESSAMENTO DE PDFs FALHADOS - NORMAS ANTAQ")
    print("=" * 60)
    
    try:
        # Encontra base de dados
        arquivo = "data/normas_antaq_completo.parquet"
        
        if not os.path.exists(arquivo):
            print("‚ùå Arquivo normas_antaq_completo.parquet n√£o encontrado")
            return
        
        print(f"üìÇ Base de dados: {arquivo}")
        
        # Analisa PDFs falhados
        falhados = analisar_pdfs_falhados(arquivo)
        
        if len(falhados) == 0:
            print(f"\n‚úÖ Nenhum PDF falhado encontrado! Nada a fazer.")
            return
        
        # Pergunta se deve continuar
        print(f"\n‚ùì Deseja reprocessar os {len(falhados)} PDFs falhados?")
        print(f"‚ö†Ô∏è  Isso pode demorar ~{len(falhados)*2/60:.0f} minutos")
        
        resposta = input("Continuar? (s/N): ").strip().lower()
        if resposta not in ['s', 'sim', 'y', 'yes']:
            print("‚ùå Reprocessamento cancelado pelo usu√°rio")
            return
        
        # Pergunta sobre limite
        limite_str = input(f"Limite de PDFs (Enter para todos os {len(falhados)}): ").strip()
        max_pdfs = None
        if limite_str:
            try:
                max_pdfs = int(limite_str)
                print(f"üéØ Limitando a {max_pdfs} PDFs")
            except ValueError:
                print("‚ö†Ô∏è Limite inv√°lido, processando todos")
        
        # Reprocessa PDFs
        df_atualizado = reprocessar_pdfs_falhados(arquivo, max_pdfs)
        
        # Salva dados atualizados
        timestamp = int(time.time())
        backup_original = f"backups/backup_antes_reprocessamento_{timestamp}.parquet"
        
        if os.path.exists(arquivo):
            os.rename(arquivo, backup_original)
            print(f"üì¶ Backup do original: {backup_original}")
        
        df_atualizado.to_parquet(arquivo, index=False)
        print(f"üíæ Dados atualizados salvos em: {arquivo}")
        
        # Estat√≠sticas finais
        total = len(df_atualizado)
        com_conteudo = df_atualizado['conteudo_pdf'].str.len().gt(0).sum()
        
        print(f"\nüìä ESTAT√çSTICAS FINAIS:")
        print(f"   üìã Total de normas: {total}")
        print(f"   üìÑ PDFs com conte√∫do: {com_conteudo}")
        print(f"   üìä Cobertura: {com_conteudo/total*100:.1f}%")
        
        print(f"\nüéâ REPROCESSAMENTO CONCLU√çDO!")
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è Reprocessamento interrompido pelo usu√°rio")
        print(f"üí° Progresso pode ter sido salvo automaticamente")
    except Exception as e:
        print(f"‚ùå Erro durante reprocessamento: {e}")

if __name__ == "__main__":
    main() 