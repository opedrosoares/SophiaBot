#!/usr/bin/env python3
"""
Script para executar extraÃ§Ã£o completa HISTÃ“RICA de todas as normas ANTAQ
Processa todos os anos desde 2002 atÃ© 2025
ATENÃ‡ÃƒO: Este processo pode demorar 12-24 horas ou mais
"""

from Scrap import SophiaANTAQScraper
import time
import os
from datetime import datetime

def main():
    print("ğŸš€ EXTRAÃ‡ÃƒO HISTÃ“RICA COMPLETA - NORMAS ANTAQ (2002-2025)")
    print("âš ï¸  ATENÃ‡ÃƒO: Este processo pode demorar 12-24 horas ou mais!")
    print("ğŸ“… SerÃ¡ executado para TODOS os anos de 2002 a 2025")
    print("ğŸ’¾ Os dados serÃ£o salvos apÃ³s cada ano processado")
    print("ğŸ’¡ Pressione Ctrl+C para interromper a qualquer momento")
    print("=" * 70)
    
    # ConfirmaÃ§Ã£o do usuÃ¡rio
    resposta = input("Deseja continuar com a extraÃ§Ã£o histÃ³rica completa? (s/N): ").strip().lower()
    if resposta not in ['s', 'sim', 'y', 'yes']:
        print("âŒ ExtraÃ§Ã£o cancelada pelo usuÃ¡rio")
        return
    
    # ConfiguraÃ§Ã£o dos anos (de 2002 a 2025)
    anos = list(range(2024, 2025))  # 2002 a 2025 inclusive
    
    print(f"\nğŸ”„ Iniciando extraÃ§Ã£o histÃ³rica para {len(anos)} anos...")
    print(f"ğŸ“… Anos a processar: {anos[0]} - {anos[-1]}")
    
    start_time_total = time.time()
    
    # ConfiguraÃ§Ãµes para extraÃ§Ã£o completa
    scraper = SophiaANTAQScraper(
        delay=1.5,  # 1.5 segundos entre requisiÃ§Ãµes (uso responsÃ¡vel)
        verify_ssl=False,
        extract_pdf_content=True  # Extrair conteÃºdo dos PDFs
    )
    
    # Nome do arquivo principal (sempre o mesmo para updates incrementais)
    arquivo_principal = "shared/data/normas_antaq_completo.parquet"
    
    # Carrega IDs existentes uma Ãºnica vez (para controle global de duplicatas)
    if os.path.exists(arquivo_principal):
        print(f"ğŸ“‚ Arquivo existente encontrado: {arquivo_principal}")
        existing_ids_global = scraper.load_existing_ids(arquivo_principal)
        print(f"ğŸ”‘ {len(existing_ids_global)} IDs jÃ¡ existentes no banco")
    else:
        print("ğŸ“‚ Iniciando nova base de dados histÃ³rica completa")
        existing_ids_global = set()
    
    # EstatÃ­sticas globais
    estatisticas = {
        'anos_processados': 0,
        'anos_com_dados': 0,
        'total_normas_encontradas': 0,
        'total_normas_novas': 0,
        'total_pdfs_extraidos': 0,
        'anos_vazios': []
    }
    
    try:
        for i, ano in enumerate(anos, 1):
            print(f"\n" + "="*60)
            print(f"ğŸ“… PROCESSANDO ANO {ano} ({i}/{len(anos)})")
            print(f"â±ï¸  Progresso total: {((i-1)/len(anos))*100:.1f}%")
            print("="*60)
            
            start_time_ano = time.time()
            
            try:
                # ObtÃ©m GUID inicial para este ano
                scraper.get_initial_guid()
                
                # Extrai TODAS as pÃ¡ginas para o ano especÃ­fico
                print(f"ğŸ“¥ Extraindo todas as normas do ano {ano}...")
                dados_ano = scraper.scrape_all_pages(max_pages=None, ano=ano)
                
                estatisticas['anos_processados'] += 1
                
                if dados_ano:
                    print(f"ğŸ” {len(dados_ano)} normas encontradas para {ano}")
                    estatisticas['total_normas_encontradas'] += len(dados_ano)
                    
                    # Filtra duplicatas baseado nos IDs globais
                    dados_novos = scraper.filter_duplicates(dados_ano, existing_ids_global)
                    
                    if dados_novos:
                        print(f"ğŸ†• {len(dados_novos)} normas NOVAS para adicionar do ano {ano}")
                        estatisticas['total_normas_novas'] += len(dados_novos)
                        estatisticas['anos_com_dados'] += 1
                        
                        # Conta PDFs extraÃ­dos
                        pdfs_extraidos = sum(1 for item in dados_novos 
                                           if item.get('conteudo_pdf', '').strip())
                        estatisticas['total_pdfs_extraidos'] += pdfs_extraidos
                        
                        # Salva dados (mescla com existentes)
                        scraper.save_to_parquet(dados_novos, arquivo_principal, merge_with_existing=True)
                        
                        # Atualiza conjunto de IDs existentes para prÃ³ximas iteraÃ§Ãµes
                        for item in dados_novos:
                            if item.get('codigo_registro'):
                                existing_ids_global.add(str(item['codigo_registro']))
                        
                        print(f"ğŸ’¾ {len(dados_novos)} normas salvas para o ano {ano}")
                        if pdfs_extraidos > 0:
                            print(f"ğŸ“„ {pdfs_extraidos} PDFs com conteÃºdo extraÃ­do")
                    else:
                        print(f"ğŸ“‹ Todas as {len(dados_ano)} normas de {ano} jÃ¡ existiam no banco")
                        
                else:
                    print(f"ğŸ“­ Nenhuma norma encontrada para o ano {ano}")
                    estatisticas['anos_vazios'].append(ano)
                
                # Tempo do ano atual
                elapsed_ano = time.time() - start_time_ano
                
                print(f"â±ï¸  Tempo para ano {ano}: {elapsed_ano/60:.1f} minutos")
                
                # Cria backup periÃ³dico a cada 5 anos processados
                if i % 5 == 0 or ano == anos[-1]:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    backup_name = f"shared/backups/backup_historico_{ano}_{timestamp}.parquet"
                    
                    # Recarrega todos os dados para o backup
                    if os.path.exists(arquivo_principal):
                        import pandas as pd
                        df_backup = pd.read_parquet(arquivo_principal)
                        df_backup.to_parquet(backup_name, engine='pyarrow', index=True)
                        print(f"ğŸ’¾ Backup criado: {backup_name} ({len(df_backup)} normas)")
                
                # Aguarda um pouco antes do prÃ³ximo ano (para nÃ£o sobrecarregar servidor)
                if i < len(anos):  # NÃ£o aguarda no Ãºltimo ano
                    print("â¸ï¸  Aguardando 3 segundos antes do prÃ³ximo ano...")
                    time.sleep(3)
                    
            except KeyboardInterrupt:
                print(f"\nâš ï¸  ExtraÃ§Ã£o interrompida pelo usuÃ¡rio no ano {ano}")
                raise
            except Exception as e:
                print(f"âŒ Erro ao processar ano {ano}: {e}")
                print("â¡ï¸  Continuando para o prÃ³ximo ano...")
                continue
        
        # EstatÃ­sticas finais
        elapsed_total = time.time() - start_time_total
        
        print(f"\n" + "="*70)
        print("ğŸ‰ EXTRAÃ‡ÃƒO HISTÃ“RICA COMPLETA FINALIZADA!")
        print("="*70)
        print(f"â±ï¸  Tempo total: {elapsed_total/3600:.1f} horas")
        print(f"ğŸ“… Anos processados: {estatisticas['anos_processados']}/{len(anos)}")
        print(f"ğŸ“Š Anos com dados: {estatisticas['anos_com_dados']}")
        print(f"ğŸ“ˆ Total de normas encontradas: {estatisticas['total_normas_encontradas']}")
        print(f"ğŸ†• Total de normas NOVAS adicionadas: {estatisticas['total_normas_novas']}")
        print(f"ğŸ“„ Total de PDFs extraÃ­dos: {estatisticas['total_pdfs_extraidos']}")
        print(f"ğŸ—ƒï¸  Total final no arquivo: {len(existing_ids_global)} normas Ãºnicas")
        print(f"ğŸ’¾ Arquivo principal: {arquivo_principal}")
        
        if estatisticas['anos_vazios']:
            print(f"ğŸ“­ Anos sem dados: {estatisticas['anos_vazios']}")
        
        if estatisticas['total_normas_encontradas'] > 0:
            velocidade = estatisticas['total_normas_encontradas'] / (elapsed_total / 3600)
            print(f"ğŸ“ˆ Velocidade mÃ©dia: {velocidade:.0f} normas/hora")
        
        # Backup final
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_final = f"shared/backups/backup_historico_completo_{timestamp}.parquet"
        
        if os.path.exists(arquivo_principal):
            import pandas as pd
            df_final = pd.read_parquet(arquivo_principal)
            df_final.to_parquet(backup_final, engine='pyarrow', index=True)
            print(f"ğŸ’¾ Backup final criado: {backup_final}")
            
            # AnÃ¡lise por dÃ©cadas
            print(f"\nğŸ“Š ANÃLISE POR DÃ‰CADAS:")
            if 'ano_norma' in df_final.columns:
                decadas = df_final['ano_norma'].value_counts().sort_index()
                for decada_inicio in range(2000, 2030, 10):
                    decada_fim = decada_inicio + 9
                    count = decadas[(decadas.index >= decada_inicio) & (decadas.index <= decada_fim)].sum()
                    if count > 0:
                        print(f"   {decada_inicio}-{decada_fim}: {count} normas")
    
    except KeyboardInterrupt:
        elapsed_parcial = time.time() - start_time_total
        print(f"\nâš ï¸  ExtraÃ§Ã£o interrompida apÃ³s {elapsed_parcial/3600:.1f} horas")
        print(f"ğŸ“Š Progresso: {estatisticas['anos_processados']}/{len(anos)} anos processados")
        print(f"ğŸ’¾ Dados parciais salvos em: {arquivo_principal}")
        
    except Exception as e:
        print(f"âŒ Erro geral durante extraÃ§Ã£o histÃ³rica: {e}")

if __name__ == "__main__":
    main()